{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgxqIWRwzTfq",
        "colab_type": "text"
      },
      "source": [
        "# **Style Transfer**\n",
        "\n",
        "Recreating the output of paper:[ Image Style Transfer Using Convolutional Neural Networks, by Gatys](https://ieeexplore.ieee.org/document/7780634)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyPC7to9zVPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60WyauX4eq57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7KejebW6Dmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(path, max_size=500, shape=None):\n",
        "  \"\"\" Load an image and transform it to a tensor (also keep pixel values <500 in x,y dimensions) \"\"\"\n",
        " \n",
        "  img = Image.open(path).convert('RGB')\n",
        "  \n",
        "  if(max(img.size) > max_size):\n",
        "    size = max_size\n",
        "  else:\n",
        "    size = max(img.size)\n",
        "  \n",
        "  if(shape is not None):\n",
        "    size = shape\n",
        "  \n",
        "  transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "  # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "  img = transform(img)[:3,:,:].unsqueeze(0)\n",
        "    \n",
        "  return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWbP-uO84TVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "for param in vgg.parameters():\n",
        "  param.requires_grad_(False)\n",
        "  \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCWU2C2m4m8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------Input images check pls------------------\n",
        "style = load_img('abstract.jpg').to(device)\n",
        "# Resize style to match content:\n",
        "content = load_img('hokusai_wave.jpg', shape=content.shape[-2:]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ereRSJvq8FzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "    \n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "# display the images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "# content and style ims side-by-side\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(style))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPxQwXlp9kJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Run an image forward through a model and get the features for \n",
        "        a set of layers. \n",
        "    \"\"\"\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1', \n",
        "                  '10': 'conv3_1', \n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',  ## content representation\n",
        "                  '28': 'conv5_1'}\n",
        "        \n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules is a dictionary holding each module in the model\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "            \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4KAA7BW8ox1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vgg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glZrOQG486Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gram_matrix(tensor):\n",
        "    \"\"\" Calculate the Gram Matrix of a given tensor \"\"\"\n",
        "    \n",
        "    # get the batch_size, depth, height, and width of the Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "    \n",
        "    # reshape so we're multiplying the features for each channel\n",
        "    tensor = tensor.view(d, h * w)\n",
        "    \n",
        "    # calculate the gram matrix\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    \n",
        "    return gram\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0I5TA3q-W_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get content and style features only once before training\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# calculate the gram matrices for each layer of our style representation\n",
        "style_grams = {layer: get_gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# create a third \"target\" image and prep it for change\n",
        "target = content.clone().requires_grad_(True).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4gXh6lw_IqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights for each style layer \n",
        "style_weights = {'conv1_1': 1.,\n",
        "                 'conv2_1': 0.75,\n",
        "                 'conv3_1': 0.2,\n",
        "                 'conv4_1': 0.2,\n",
        "                 'conv5_1': 0.2}\n",
        "\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e6  # beta\n",
        "\n",
        "#alpha/beta ratio should be large so content image is more dominant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLaETX-d_Zoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for displaying the target image, intermittently\n",
        "show_every = 500\n",
        "\n",
        "# iteration hyperparameters\n",
        "optimizer = optim.Adam([target], lr=0.003)\n",
        "steps = 3500  # iterations to update image\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "    \n",
        "    # get the features from target image\n",
        "    target_features = get_features(target, vgg)\n",
        "    \n",
        "    # the content loss\n",
        "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "    \n",
        "    # the style loss\n",
        "    # initialize the style loss to 0\n",
        "    style_loss = 0\n",
        "    # then add to it for each layer's gram matrix loss\n",
        "    for layer in style_weights:\n",
        "        # get the target style representation for the layer\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = get_gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "        # get the style representation\n",
        "        style_gram = style_grams[layer]\n",
        "        # the style loss for one layer, weighted \n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        # add to the style loss\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "        \n",
        "    # calculate the total loss\n",
        "    total_loss = (content_weight * content_loss) + (style_weight * style_loss)\n",
        "    \n",
        "    # update target image\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # display intermediate images and print the loss\n",
        "    if  ii % show_every == 0:\n",
        "        print('Total loss: ', total_loss.item())\n",
        "        plt.imshow(im_convert(target))\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuwxQK5v_6VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display content and final, target image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl_AwEDIzxOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}